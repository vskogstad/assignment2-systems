#model configuration
model_name: "transformer"
vocab_size: 10000 # 32768
num_layers: 2
num_heads: 1
d_model: 32
d_ff: 128
context_length: 128
theta: 10000
pre_norm: true
layer_norm: true
glu: true
num_kv_groups: null

# data loading
train_dataset_path: "data/training_data/owt_train.npy"
val_dataset_path: "data/training_data/owt_valid.npy"
tokenizer_vocab_path: "data/tokenizer_data/vocab_owt_train.pkl"
tokenizer_merges_path: "data/tokenizer_data/merges_owt_train.pkl"
#sample_without_replacement: true

# Training configuration
dtype: "float32"
batch_size: 4
min_learning_rate: 1.0e-5
max_learning_rate: 0.0050
weight_decay: 0.1
warmup_steps: 600
total_steps: 15800
cosine_cycle_steps: 15800
betas: [0.9, 0.999]
eps: 1.0e-8
muon_momentum: 0.95
grad_clip_norm: 1.0
optimizer: "adamw"
scheduler: "cosine"
compile_model: false

# Experiment configuration
experiment_name: "Benchmarking and profiling"
seed: 42
log_interval: 10
eval_interval: 3000
save_interval: 16100
output_dir: "cs336_basics/configs/experiments"
wandb_project: "H100-sxm5-OWT-testruns"
from_checkpoint: null #"cs336_basics/checkpoints/baseline_2000.pth"