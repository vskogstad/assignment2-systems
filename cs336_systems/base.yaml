#model configuration
model_name: "transformer"
vocab_size: 32000 # 32768
num_layers: 2
num_heads: 1
d_model: 32
d_ff: 128
context_length: 32
theta: 10000
pre_norm: true
layer_norm: true
glu: true
num_kv_groups: null

# data loading
train_dataset_path: "data/training_data/owt_train.npy"
val_dataset_path: "data/training_data/owt_valid.npy"
tokenizer_vocab_path: "data/tokenizer_data/vocab_owt_train.pkl"
tokenizer_merges_path: "data/tokenizer_data/merges_owt_train.pkl"
#sample_without_replacement: true

# Training configuration
dtype: float32 # bfloat16
batch_size: 4
min_learning_rate: 1.0e-5
max_learning_rate: 0.0050
weight_decay: 0.1
warmup_steps: 600
total_steps: 10501
cosine_cycle_steps: 10501
betas: [0.9, 0.95]
eps: 1.0e-8
muon_momentum: 0.95
grad_clip_norm: 1.0
optimizer: "muon"
scheduler: "cosine"

# Experiment configuration
experiment_name: "without_repeated_sampling"
seed: 42
log_interval: 10
eval_interval: 10500
save_interval: 10600
output_dir: "cs336_basics/configs/experiments"
wandb_project: H100-sxm5-OWT-testruns
from_checkpoint: null #"cs336_basics/checkpoints/baseline_2000.pth"