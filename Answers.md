**Benchmarking script**


b)

In general the forward pass takes half the time of the backward pass, as is expected. As the size of the matrices increases, the time to calculate each pass increases. For 2.7B we get out of memory errors. Standard deviation is small, slightly higher for forward pass, but that could be due to lower total time. 

+-------------------+----------------+---------------+-----------------+----------------+
| Experiment_name   |   Forward time |   Forward std |   backward time |   backward std |
+===================+================+===============+=================+================+
| small             |      0.0232807 |   1.94524e-05 |       0.0457747 |    1.09413e-05 |
+-------------------+----------------+---------------+-----------------+----------------+
| medium            |      0.0797462 |   0.000106837 |       0.160158  |    2.3159e-05  |
+-------------------+----------------+---------------+-----------------+----------------+
| large             |      0.239775  |   8.18561e-05 |       0.469623  |    4.1262e-05  |
+-------------------+----------------+---------------+-----------------+----------------+
| xl                |      0.596514  |   0.000358874 |       1.17029   |    2.77922e-05 |
+-------------------+----------------+---------------+-----------------+----------------+

c)

With no warmup we get wildly varying forward time especially as the first initialization massively impacts the forward time and forward std. 
+-------------------+----------------+---------------+-----------------+----------------+
| Experiment_name   |   Forward time |   Forward std |   backward time |   backward std |
+===================+================+===============+=================+================+
| small             |        1.22569 |       3.60679 |        0.133036 |    0.261757    |
+-------------------+----------------+---------------+-----------------+----------------+
| medium            |        4.99526 |      14.7463  |        0.16031  |    0.000396688 |
+-------------------+----------------+---------------+-----------------+----------------+
| large             |        7.80862 |      22.7059  |        1.25489  |    2.35576     |
+-------------------+----------------+---------------+-----------------+----------------+
| xl                |       10.6257  |      30.0867  |        2.26297  |    3.27795     |
+-------------------+----------------+---------------+-----------------+----------------+

With 1 warmup step:
+-------------------+----------------+---------------+-----------------+----------------+
| Experiment_name   |   Forward time |   Forward std |   backward time |   backward std |
+===================+================+===============+=================+================+
| small             |      0.0233525 |   0.000138351 |       0.0457912 |    1.89401e-05 |
+-------------------+----------------+---------------+-----------------+----------------+
| medium            |      0.0798658 |   9.14374e-05 |       0.16022   |    0.000166551 |
+-------------------+----------------+---------------+-----------------+----------------+
| large             |      0.240096  |   0.000268037 |       0.469644  |    6.33816e-05 |
+-------------------+----------------+---------------+-----------------+----------------+
| xl                |      0.597181  |   0.00171275  |       1.17032   |    7.78347e-05 |
+-------------------+----------------+---------------+-----------------+----------------+

With 2 warmup steps we basically have the same time and std as with 5. According to torch.compile manual we need 2 warmup steps for CUDA graphs.
+-------------------+----------------+---------------+-----------------+----------------+
| Experiment_name   |   Forward time |   Forward std |   backward time |   backward std |
+===================+================+===============+=================+================+
| small             |      0.023164  |   4.72463e-05 |       0.0455083 |    1.80883e-05 |
+-------------------+----------------+---------------+-----------------+----------------+
| medium            |      0.0793609 |   0.000141764 |       0.159249  |    0.000228875 |
+-------------------+----------------+---------------+-----------------+----------------+
| large             |      0.239858  |   0.000101835 |       0.469591  |    4.35804e-05 |
+-------------------+----------------+---------------+-----------------+----------------+
| xl                |      0.596489  |   0.000230396 |       1.17028   |    1.83074e-05 |
+-------------------+----------------+---------------+-----------------+----------------+

Nsys_profile:
a)
With profiling nsys we see that we spend fomr 3x to 1.2x longer in the forward pass. The change is smaller for bigger models. Backward pass is unchanged.
+-------------------+----------------+---------------+-----------------+----------------+
| Experiment_name   |   Forward time |   Forward std |   backward time |   backward std |
+===================+================+===============+=================+================+
| small             |      0.0667576 |    0.00174992 |        0.046532 |    2.32975e-05 |
+-------------------+----------------+---------------+-----------------+----------------+
| medium            |      0.164362  |    0.0022177  |        0.162123 |    0.000118179 |
+-------------------+----------------+---------------+-----------------+----------------+
| large             |      0.364349  |    0.00146652 |        0.473269 |    0.000272336 |
+-------------------+----------------+---------------+-----------------+----------------+
| xl                |      0.762566  |    0.0026405  |        1.1778   |    0.000947067 |
+-------------------+----------------+---------------+-----------------+----------------+

b) 
Found that I had used torch.compile for everything so far. I have adjusted that now, and also made changes to my model which will affect timings. This is the new standard time used.

c)

d)

e)


Mixed precision:

The resulting tensors are:
tensor(10.0001)                         -> Pure float32, expecting high accuracy
tensor(9.9531, dtype=torch.float16)     -> Pure float16. Lower precision -> missing target over time.
tensor(10.0021)                         -> Small number in float16. Accumulate result in float32.
tensor(10.0021)                         -> Shows that adding a float16 value to float32 accumulator is the same as explicitly casting float16 to float32 and adding.

